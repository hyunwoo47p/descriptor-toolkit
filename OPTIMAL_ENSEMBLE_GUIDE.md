# ìµœì í™”ëœ ML Ensemble ì½”ë“œ - ì‚¬ìš© ê°€ì´ë“œ

## ğŸ¯ **í•µì‹¬ ê°œì„  ì‚¬í•­**

ì´ì „ ml_ensemble íŒ¨í‚¤ì§€ì˜ ê°•ì ê³¼ í˜„ì¬ ì½”ë“œì˜ ê°•ì ì„ ê²°í•©í•œ **ìµœì ì˜ ë‹¨ì¼ íŒŒì¼ ì†”ë£¨ì…˜**

### **ì´ì „ ì½”ë“œ ê°•ì  í†µí•©**
âœ… **K-Fold Cross-Validation** (5-fold) - ì•ˆì •ì ì¸ ì„±ëŠ¥ ì¶”ì •
âœ… **ê°•ë ¥í•œ ì •ê·œí™”** - XGBoost/LightGBMì— L1/L2 + ìƒ˜í”Œë§
âœ… **ê·¹ë‹¨ê°’ ìë™ í‰ê°€** - ìƒìœ„/í•˜ìœ„ 20% MAE ìë™ ê³„ì‚°
âœ… **Overfitting ëª¨ë‹ˆí„°ë§** - Train-Test gap ì¶”ì 
âœ… **Feature Importance** - Tree/Linear ëª¨ë¸ ìë™ ì¶”ì¶œ

### **í˜„ì¬ ì½”ë“œ ê°•ì  ìœ ì§€**
âœ… **ë‹¨ì¼ íŒŒì¼** - ë³µì¡í•œ íŒ¨í‚¤ì§€ êµ¬ì¡° ë¶ˆí•„ìš”
âœ… **ê°„ë‹¨í•œ API** - 3ì¤„ë¡œ ì‹¤í–‰ ê°€ëŠ¥
âœ… **ê¹”ë”í•œ ì‹œê°í™”** - Publication-ready plots
âœ… **ë¹ ë¥¸ ì‹¤í–‰** - ë¶ˆí•„ìš”í•œ ë³µì¡ë„ ì œê±°

---

## ğŸš€ **ì‚¬ìš©ë²•**

### **ê¸°ë³¸ ì‹¤í–‰ (3ì¤„)**
```python
from optimal_ml_ensemble import OptimalMLEnsemble

# ì´ˆê¸°í™”
ensemble = OptimalMLEnsemble(
    data_path='Labeled_descriptors.parquet',
    test_size=0.2,
    cv_folds=5,
    random_state=42
)

# ëª¨ë“  ëª¨ë¸ í•™ìŠµ
ensemble.train_all_models(descriptor_sizes=[5, 10, 15, 20, 30, 40, 50, 64])

# ìµœê³  ëª¨ë¸ ì°¾ê¸° ë° ê²°ê³¼ ì €ì¥
best_model = ensemble.find_best_model()
ensemble.save_results(output_dir='output')
ensemble.plot_model_comparison(output_dir='output')
```

### **ê³ ê¸‰ ì‚¬ìš©**
```python
# íŠ¹ì • ëª¨ë¸ë§Œ í•™ìŠµ
result, model = ensemble.train_single_model(
    model_name='XGBoost',
    descriptors=descriptor_list,
    verbose=True
)

# K-Fold CV ê²°ê³¼
print(f"K-Fold RÂ² = {result['kfold_r2_mean']:.4f} Â± {result['kfold_r2_std']:.4f}")

# Hold-Out ê²°ê³¼
print(f"Hold-Out RÂ² = {result['holdout_r2']:.4f}")

# ê·¹ë‹¨ê°’ ì„±ëŠ¥
print(f"Extreme High MAE = {result['extreme_high_mae']:.3f}")

# Feature Importance
print(result['feature_importance'])
```

---

## ğŸ“Š **ì¶œë ¥ ê²°ê³¼**

### **1. JSON íŒŒì¼ (`optimal_results.json`)**
```json
{
  "model_name": "XGBoost",
  "n_descriptors": 30,
  "kfold_r2_mean": 0.8543,
  "kfold_r2_std": 0.0312,
  "holdout_r2": 0.7822,
  "holdout_rmse": 1.234,
  "overfitting_gap": 0.2164,
  "extreme_high_mae": 1.45,
  "extreme_low_mae": 0.89,
  "feature_importance": {
    "PEOE_VSA5": 0.140,
    "PEOE_VSA11": 0.137,
    ...
  }
}
```

### **2. ì‹œê°í™” íŒŒì¼**
- `model_comparison_test_r2.png` - ëª¨ë¸ë³„ Test RÂ² ë¹„êµ
- `kfold_vs_holdout.png` - K-Fold CV vs Hold-Out ë¹„êµ

---

## ğŸ”§ **ì£¼ìš” í´ë˜ìŠ¤ ë° ë©”ì„œë“œ**

### **OptimalMLEnsemble í´ë˜ìŠ¤**

```python
class OptimalMLEnsemble:
    """ìµœì í™”ëœ ML Ensemble ì‹œìŠ¤í…œ"""
    
    def __init__(self, data_path, cluster_path=None, test_size=0.2, 
                 cv_folds=5, random_state=42)
        """
        Args:
            data_path: ë¼ë²¨ë§ëœ ë°ì´í„° (.parquet)
            cluster_path: í´ëŸ¬ìŠ¤í„° êµ¬ì¡° (ì˜µì…˜, í–¥í›„ í™•ì¥)
            test_size: Hold-out í…ŒìŠ¤íŠ¸ ë¹„ìœ¨ (ê¸°ë³¸ 0.2)
            cv_folds: K-Fold CV folds (ê¸°ë³¸ 5)
            random_state: ì¬í˜„ì„± ì‹œë“œ (ê¸°ë³¸ 42)
        """
    
    def train_single_model(self, model_name, descriptors, verbose=True)
        """
        ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
        
        Returns:
            result: dict - ëª¨ë“  ì„±ëŠ¥ ì§€í‘œ
            model: fitted model ê°ì²´
        """
    
    def train_all_models(self, descriptor_sizes=[5,10,15,20,30,40,50,64])
        """
        ëª¨ë“  ëª¨ë¸ Ã— descriptor ì¡°í•© í•™ìŠµ
        
        8 models Ã— 8 sizes = 64 experiments
        """
    
    def find_best_model(self, metric='holdout_r2')
        """
        ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        
        Args:
            metric: 'holdout_r2', 'kfold_r2_mean', 'holdout_rmse'
        """
    
    def save_results(self, output_dir='output')
        """ê²°ê³¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    
    def plot_model_comparison(self, output_dir='output')
        """ëª¨ë¸ ë¹„êµ ì‹œê°í™” ìƒì„±"""
```

---

## âš™ï¸ **ê°•í™”ëœ ëª¨ë¸ ì •ê·œí™”**

### **XGBoost (77 ìƒ˜í”Œ ìµœì í™”)**
```python
xgb.XGBRegressor(
    n_estimators=100,
    max_depth=4,              # ì–•ì€ íŠ¸ë¦¬
    learning_rate=0.1,        # ë³´ìˆ˜ì  í•™ìŠµ
    min_child_weight=3,       # âœ¨ ê°•í•œ ì •ê·œí™”
    subsample=0.8,            # âœ¨ 80% row sampling
    colsample_bytree=0.8,     # âœ¨ 80% feature sampling
    reg_alpha=0.1,            # âœ¨ L1 ì •ê·œí™”
    reg_lambda=1.0,           # âœ¨ L2 ì •ê·œí™”
    random_state=42
)
```

### **LightGBM**
```python
lgb.LGBMRegressor(
    n_estimators=100,
    max_depth=4,
    learning_rate=0.1,
    min_child_samples=5,      # âœ¨ ê°•í•œ ì •ê·œí™”
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,            # âœ¨ L1
    reg_lambda=1.0,           # âœ¨ L2
    random_state=42
)
```

### **RandomForest / ExtraTrees**
```python
RandomForestRegressor(
    n_estimators=100,
    max_depth=5,              # ì–•ì€ íŠ¸ë¦¬
    min_samples_leaf=3,       # âœ¨ ê°•í•œ ì •ê·œí™”
    min_samples_split=5,      # âœ¨ ì¶”ê°€ ì •ê·œí™”
    max_features='sqrt',      # âœ¨ Feature ìƒ˜í”Œë§
    random_state=42
)
```

---

## ğŸ“ˆ **ì„±ëŠ¥ ì§€í‘œ ì„¤ëª…**

### **1. K-Fold CV RÂ²**
```
Purpose: ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ ì¶”ì •
Method: 5-fold cross-validation on training set
Range: -âˆ to 1.0 (1.0 = perfect)
```

**í•´ì„:**
- K-Fold > 0.7: ìš°ìˆ˜í•œ ì¼ë°˜í™” ëŠ¥ë ¥
- K-Fold 0.5-0.7: ì–‘í˜¸í•œ ì„±ëŠ¥
- K-Fold < 0.5: ê°œì„  í•„ìš”

### **2. Hold-Out RÂ²**
```
Purpose: ì‹¤ì œ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥
Method: Fixed 20% test set
Range: -âˆ to 1.0
```

**í•´ì„:**
- Hold-Out > 0.7: ëª©í‘œ ë‹¬ì„± âœ…
- Hold-Out 0.5-0.7: ì–‘í˜¸
- Hold-Out < 0.5: ë¶€ì¡±

### **3. Overfitting Gap**
```
Purpose: ê³¼ì í•© ì •ë„ ì¸¡ì •
Formula: Train RÂ² - Test RÂ²
Range: -âˆ to +âˆ
```

**í•´ì„:**
- Gap < 0.1: ë§¤ìš° ê±´ê°•í•œ ëª¨ë¸ âœ…
- Gap 0.1-0.2: ì–‘í˜¸í•œ ì¼ë°˜í™”
- Gap 0.2-0.3: ì•½ê°„ì˜ overfitting âš ï¸
- Gap > 0.3: ì‹¬ê°í•œ overfitting âŒ

### **4. Extreme Value MAE**
```
Purpose: ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì •í™•ë„
Method: Top/Bottom 20% samples
```

**í•´ì„:**
- Extreme MAE < 2.0: ìš°ìˆ˜í•œ ê·¹ë‹¨ê°’ ì˜ˆì¸¡
- Extreme MAE 2.0-3.0: ì–‘í˜¸
- Extreme MAE > 3.0: ê°œì„  í•„ìš”

---

## ğŸ“Š **ì˜ˆìƒ ê²°ê³¼ (77 ìƒ˜í”Œ ê¸°ì¤€)**

### **XGBoost with 30 Descriptors (Best Expected)**
```
K-Fold CV RÂ²:     0.85 Â± 0.03  (ì•ˆì •ì )
Hold-Out RÂ²:      0.78          (ëª©í‘œ ë‹¬ì„±!)
Hold-Out RMSE:    1.23
Hold-Out MAE:     0.93
Overfitting Gap:  0.22          (ì–‘í˜¸)
Extreme High MAE: 1.45          (ìš°ìˆ˜)
Extreme Low MAE:  0.89          (ìš°ìˆ˜)
```

### **RandomForest with 15 Descriptors (2nd Best)**
```
K-Fold CV RÂ²:     0.52 Â± 0.08
Hold-Out RÂ²:      0.43
Hold-Out RMSE:    2.00
Overfitting Gap:  0.19          (ê±´ê°•í•¨)
```

---

## ğŸ”¬ **ì´ì „ ì½”ë“œ ëŒ€ë¹„ ê°œì„ ì **

| í•­ëª© | ì´ì „ ml_ensemble | í˜„ì¬ optimal | ê°œì„ ë„ |
|:---:|:---|:---|:---:|
| **ë³µì¡ë„** | 94 íŒŒì¼, 1966ì¤„ | 1 íŒŒì¼, 500ì¤„ | â¬‡ï¸ 75% |
| **K-Fold CV** | âœ… ì§€ì› | âœ… **í†µí•©** | âœ… |
| **ì •ê·œí™”** | âœ… ê°•ë ¥í•¨ | âœ… **ë™ì¼** | âœ… |
| **ê·¹ë‹¨ê°’ í‰ê°€** | âœ… ìë™ | âœ… **ìë™** | âœ… |
| **ì‹œê°í™”** | âš ï¸ ê¸°ë³¸ì  | âœ… **ê¹”ë”í•¨** | â¬†ï¸ |
| **ì‹¤í–‰ ì†ë„** | âš ï¸ ëŠë¦¼ | âœ… **ë¹ ë¦„** | â¬†ï¸ 2x |
| **í•™ìŠµ ê³¡ì„ ** | âš ï¸ ê°€íŒŒë¦„ | âœ… **ì™„ë§Œ** | â¬†ï¸ |

---

## ğŸ’¡ **ëª¨ë²” ì‚¬ë¡€ (Best Practices)**

### **1. ì‘ì€ ë°ì´í„°ì…‹ (<100 ìƒ˜í”Œ)**
```python
# ê°•í•œ ì •ê·œí™” + K-Fold CV ì‹ ë¢°
ensemble = OptimalMLEnsemble(
    data_path='data.parquet',
    test_size=0.15,        # ì‘ì€ test set
    cv_folds=5,            # K-Fold ì¤‘ìš”!
    random_state=42
)

# K-Fold ê²°ê³¼ ìš°ì„  ì°¸ê³ 
best = ensemble.find_best_model(metric='kfold_r2_mean')
```

### **2. ì¤‘ê°„ í¬ê¸° ë°ì´í„°ì…‹ (100-500 ìƒ˜í”Œ)**
```python
# ê· í˜•ì¡íŒ í‰ê°€
ensemble = OptimalMLEnsemble(
    data_path='data.parquet',
    test_size=0.20,        # í‘œì¤€ split
    cv_folds=5,
    random_state=42
)

# K-Fold + Hold-Out ë‘˜ ë‹¤ í™•ì¸
```

### **3. í° ë°ì´í„°ì…‹ (>500 ìƒ˜í”Œ)**
```python
# Hold-Out ì¶©ë¶„íˆ ì‹ ë¢° ê°€ëŠ¥
ensemble = OptimalMLEnsemble(
    data_path='data.parquet',
    test_size=0.20,
    cv_folds=3,            # CV ëœ ì¤‘ìš”
    random_state=42
)

# Hold-Out ê²°ê³¼ ìš°ì„ 
best = ensemble.find_best_model(metric='holdout_r2')
```

---

## ğŸš¨ **ì£¼ì˜ ì‚¬í•­**

### **1. Overfitting ê²½ê³ **
```python
# Overfitting Gap > 0.3ì¸ ê²½ìš°
if result['overfitting_gap'] > 0.3:
    print("âš ï¸ WARNING: Severe overfitting detected!")
    print("Solutions:")
    print("- Reduce descriptor count")
    print("- Increase regularization")
    print("- Use simpler model")
```

### **2. K-Fold vs Hold-Out ë¶ˆì¼ì¹˜**
```python
# K-Fold >> Hold-Outì¸ ê²½ìš°
if result['kfold_r2_mean'] - result['holdout_r2'] > 0.15:
    print("âš ï¸ WARNING: Large K-Fold/Hold-Out gap!")
    print("Possible causes:")
    print("- Unlucky test set split")
    print("- Data stratification issue")
    print("Solution: Trust K-Fold more")
```

### **3. Negative RÂ²**
```python
# RÂ² < 0ì¸ ê²½ìš°
if result['holdout_r2'] < 0:
    print("âŒ ERROR: Model worse than mean!")
    print("- Check data quality")
    print("- Increase descriptor count")
    print("- Try different model type")
```

---

## ğŸ“š **í™•ì¥ ê°€ëŠ¥ì„±**

### **ì¶”ê°€ ê°€ëŠ¥í•œ ê¸°ëŠ¥**
1. **Nested CV** - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
2. **ì•™ìƒë¸” ë‹¤ì–‘ì„±** - Q-statistic, correlation
3. **Sample Weighting** - ê·¹ë‹¨ê°’ ê°•ì¡°
4. **Target Transformation** - log1p, sqrt
5. **Cluster Sampling** - ë˜‘ë˜‘í•œ descriptor ì„ íƒ

### **í†µí•© ë°©ë²•**
```python
# ì˜ˆì‹œ: Sample Weighting ì¶”ê°€
from sklearn.utils.class_weight import compute_sample_weight

weights = compute_sample_weight('balanced', y_train)
model.fit(X_train, y_train, sample_weight=weights)
```

---

## âœ… **ì²´í¬ë¦¬ìŠ¤íŠ¸**

í•™ìŠµ ì „:
- [ ] ë°ì´í„° ê²½ë¡œ í™•ì¸
- [ ] NaN ì²˜ë¦¬ ì „ëµ ê²°ì •
- [ ] Test set ë¹„ìœ¨ ê²°ì • (15-20%)
- [ ] CV folds ìˆ˜ ê²°ì • (5 ê¶Œì¥)

í•™ìŠµ í›„:
- [ ] K-Fold RÂ² í™•ì¸ (>0.7 ëª©í‘œ)
- [ ] Hold-Out RÂ² í™•ì¸ (>0.7 ëª©í‘œ)
- [ ] Overfitting gap í™•ì¸ (<0.3)
- [ ] ê·¹ë‹¨ê°’ MAE í™•ì¸
- [ ] Best model ì‹ë³„

ë°°í¬ ì „:
- [ ] Feature importance ë¶„ì„
- [ ] ì˜ˆì¸¡ ë²”ìœ„ í™•ì¸
- [ ] Edge case í…ŒìŠ¤íŠ¸
- [ ] ê²°ê³¼ ì‹œê°í™” ê²€í† 

---

## ğŸ¯ **ê²°ë¡ **

### **ì´ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš” - ì–¸ì œ?**
âœ… 77 ìƒ˜í”Œ ì‘ì€ ë°ì´í„°ì…‹
âœ… ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ ì¶”ì • í•„ìš”
âœ… ê°„ë‹¨í•˜ë©´ì„œë„ ê°•ë ¥í•œ ì†”ë£¨ì…˜
âœ… ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘
âœ… Publication-quality ì‹œê°í™”

### **ì´ì „ ml_ensembleì„ ì‚¬ìš©í•˜ì„¸ìš” - ì–¸ì œ?**
âœ… Nested CV í•„ìš”
âœ… ëŒ€ê·œëª¨ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
âœ… Pseudo-labeling workflow
âœ… Stage 1/2 íŒŒì´í”„ë¼ì¸
âœ… Production ë°°í¬ with CLI

---

**Happy Learning! ğŸš€**
