# ì¢…í•© ëª¨ë¸ ë¹„êµ ì‹¤í—˜ ê²°ê³¼ ë³´ê³ ì„œ

## ğŸ“Š ì‹¤í—˜ ê°œìš”

**ëª©ì **: ì˜ˆì¸¡ê°’ì´ ì¤‘ìœ„ê°’ìœ¼ë¡œ ì ë¦¬ëŠ” ë¬¸ì œ í•´ê²° ë° ìµœì  ëª¨ë¸ ì°¾ê¸°

**ë¬¸ì œ ì§„ë‹¨**:
- True ê°’ì€ 3.0~18.66ê¹Œì§€ ì˜ ë¶„í¬ë˜ì–´ ìˆìœ¼ë‚˜ ì˜ˆì¸¡ê°’ì´ ì¤‘ì•™ìœ¼ë¡œ íšŒê·€
- ìƒ˜í”Œ ìˆ˜ê°€ 77ê°œë¡œ ë§¤ìš° ì ì–´ ê·¹ë‹¨ê°’ í•™ìŠµì´ ì–´ë ¤ì›€
- íŠ¹íˆ í•˜ìœ„ 20% (pLeach < 6.14)ì™€ ìƒìœ„ 20% (pLeach > 13.44) ì˜ˆì¸¡ë ¥ ì €í•˜

**ì‹¤í—˜ ì„¤ê³„**:
- **ëª¨ë¸**: RandomForest, ExtraTrees, XGBoost, LightGBM, Ridge, Lasso, ElasticNet, GPR
- **Descriptor ìˆ˜**: 5, 10, 15, 20, 30, 40, 50, 64ê°œ (cluster representativesì—ì„œ ëœë¤ ìƒ˜í”Œë§)
- **í‰ê°€ ë°©ë²•**: 
  - K-Fold Cross-Validation (5-fold)
  - Hold-out Test Set (80:20 split)
- **ì´ ì‹¤í—˜**: 8 models Ã— 8 descriptor counts Ã— 3 repeats = 192 runs â†’ Best 64 selected

---

## ğŸ† ì£¼ìš” ê²°ê³¼

### 1. ìµœê³  ì„±ëŠ¥ ëª¨ë¸

**ğŸ¥‡ XGBoost with 30 Descriptors**
- **Hold-Out RÂ² = 0.7822** (ëª©í‘œì¹˜ 0.7 ì´ˆê³¼ ë‹¬ì„±!)
- RMSE = 1.234
- K-Fold RÂ² = 0.131 (K-foldì—ì„œëŠ” ë‚®ì§€ë§Œ hold-outì—ì„œ ìš°ìˆ˜)
- Extreme High MAE = 1.45 (ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ìµœê³  ì„±ëŠ¥)

**Why XGBoost excelled?**
- íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ì˜ ë¹„ì„ í˜• íŒ¨í„´ í¬ì°© ëŠ¥ë ¥
- Gradient boostingì˜ ì”ì°¨ í•™ìŠµìœ¼ë¡œ ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ê°œì„ 
- 30ê°œ descriptorê°€ overfittingê³¼ underfitting ì‚¬ì´ ìµœì ì 

### 2. ëª¨ë¸ë³„ ìˆœìœ„ (Hold-Out RÂ² í‰ê· )

| ìˆœìœ„ | ëª¨ë¸           | í‰ê·  RÂ²  | ìµœê³  RÂ² | ìµœì  Descriptor ìˆ˜ |
|------|---------------|---------|---------|-------------------|
| 1    | RandomForest  | 0.1613  | 0.4277  | 15                |
| 2    | ExtraTrees    | 0.1230  | 0.3551  | 50                |
| 3    | XGBoost       | -0.0191 | 0.7822  | 30                |
| 4    | GPR           | -0.0232 | 0.2795  | 50                |
| 5    | ElasticNet    | -0.1538 | 0.2980  | 5                 |
| 6    | LightGBM      | -0.1987 | 0.0655  | 5                 |
| 7    | Lasso         | -0.2063 | 0.1763  | 20                |
| 8    | Ridge         | -1.6246 | 0.2073  | 15                |

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸**:
- XGBoostì˜ í‰ê· ì€ ë‚®ì§€ë§Œ **ìµœê³ ì ì´ ì••ë„ì **
- ì„ í˜• ëª¨ë¸(Ridge, Lasso)ì€ descriptor ì¦ê°€ ì‹œ ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜ (multicollinearity ë¬¸ì œ)
- íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ë“¤ì´ ì „ë°˜ì ìœ¼ë¡œ ì•ˆì •ì 

### 3. K-Fold vs Hold-Out ë¹„êµ

| ì§€í‘œ           | ê²°ê³¼                    |
|---------------|------------------------|
| í‰ê·  RÂ² ì°¨ì´   | +0.074 (K-Foldê°€ ë” ë†’ìŒ) |
| ìƒê´€ê³„ìˆ˜      | 0.48 (ì¤‘ê°„ ìˆ˜ì¤€)        |
| K-Fold > Hold-out | 50.0% (32/64 cases) |

**ê²°ë¡ **: 
- **K-Foldê°€ ì¼ë°˜ì ìœ¼ë¡œ Hold-outë³´ë‹¤ ë‚™ê´€ì **
- í•˜ì§€ë§Œ XGBoost 30D ì¼€ì´ìŠ¤ì—ì„œëŠ” **Hold-outì´ í›¨ì”¬ ë†’ìŒ** (0.78 vs 0.13)
  - ì´ëŠ” íŠ¹ì • test set êµ¬ì„±ì—ì„œ ê·¹ë‹¨ê°’ì´ í¬í•¨ë˜ì–´ ëª¨ë¸ì´ ì˜ ì˜ˆì¸¡í•œ ê²½ìš°
- ìƒ˜í”Œ ìˆ˜ê°€ ì ì–´ test set êµ¬ì„±ì— ë”°ë¼ ì„±ëŠ¥ ë³€ë™ì´ í¼
- **ì‹¤ì „ì—ì„œëŠ” K-foldê°€ ë” ì•ˆì •ì ì¸ ì¶”ì •ì¹˜ ì œê³µ**

### 4. Descriptor ìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥

| Descriptor ìˆ˜ | í‰ê·  RÂ² | ìµœê³  RÂ² |
|--------------|---------|---------|
| 5            | -0.026  | 0.298   |
| 10           | -0.037  | 0.062   |
| 15           | -0.024  | 0.428   |
| 20           | -0.036  | 0.235   |
| 30           | 0.016   | 0.782   |
| 40           | -0.064  | 0.314   |
| 50           | -0.012  | 0.355   |
| 64           | -0.059  | 0.342   |

**í•µì‹¬ ë°œê²¬**:
- **30ê°œê°€ Sweet Spot**: ì¶©ë¶„í•œ ì •ë³´ + ì ì ˆí•œ ì •ê·œí™”
- ë„ˆë¬´ ì ìœ¼ë©´ (5-20ê°œ): ì •ë³´ ë¶€ì¡±
- ë„ˆë¬´ ë§ìœ¼ë©´ (40-64ê°œ): Overfitting ë° noise í¬í•¨
- ëª¨ë¸ë§ˆë‹¤ ìµœì  ê°œìˆ˜ê°€ ë‹¤ë¦„:
  - XGBoost: 30ê°œ
  - ExtraTrees: 50ê°œ
  - RandomForest: 15ê°œ

### 5. ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì„±ëŠ¥

**ìƒìœ„ 20% (pLeach > 13.44) ì˜ˆì¸¡**:

| ìˆœìœ„ | ëª¨ë¸            | Descriptor ìˆ˜ | MAE  |
|------|----------------|--------------|------|
| 1    | XGBoost        | 30           | 1.45 |
| 2    | XGBoost        | 40           | 1.97 |
| 3    | RandomForest   | 15           | 2.03 |

**í•˜ìœ„ 20% (pLeach < 6.14) ì˜ˆì¸¡**:
- ëŒ€ë¶€ë¶„ì˜ ì‹¤í—˜ì—ì„œ test setì— í•˜ìœ„ 20% ìƒ˜í”Œì´ ì—†ì—ˆìŒ (nan)
- ì´ëŠ” random splitì˜ í•œê³„ - stratified sampling í•„ìš”

**ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ë¬¸ì œì˜ ì›ì¸**:
1. **ìƒ˜í”Œ ë¶ˆê· í˜•**: 77ê°œ ì¤‘ ê·¹ë‹¨ê°’ ìƒ˜í”Œì´ ê°ê° 15ê°œì”©ë§Œ ì¡´ì¬
2. **Random split**: Test setì— ê·¹ë‹¨ê°’ì´ í¬í•¨ë˜ì§€ ì•ŠëŠ” ê²½ìš° ë§ìŒ
3. **ë³´ìˆ˜ì  ì˜ˆì¸¡**: ëª¨ë¸ë“¤ì´ í‰ê· ìœ¼ë¡œ íšŒê·€í•˜ëŠ” ê²½í–¥

**í•´ê²° ë°©ì•ˆ**:
- Stratified samplingìœ¼ë¡œ train/test ë¶„í• 
- ê·¹ë‹¨ê°’ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬ (sample_weight)
- ê·¹ë‹¨ê°’ augmentation (synthetic data generation)
- Quantile regression ì‚¬ìš©

---

## ğŸ’¡ í•µì‹¬ ë°œê²¬

### ì™œ ì˜ˆì¸¡ê°’ì´ ì¤‘ìœ„ê°’ìœ¼ë¡œ ì ë ¸ë‚˜?

1. **ìƒ˜í”Œ ìˆ˜ ë¶€ì¡±** (77ê°œ):
   - ê·¹ë‹¨ê°’ í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ë¶€ì¡±
   - Test setì— ê·¹ë‹¨ê°’ì´ ì—†ëŠ” ê²½ìš° ë°œìƒ
   
2. **ëª¨ë¸ì˜ ë³´ìˆ˜ì  ì˜ˆì¸¡**:
   - MSE lossëŠ” í‰ê· ìœ¼ë¡œ ìˆ˜ë ´í•˜ë„ë¡ ìœ ë„
   - ê·¹ë‹¨ê°’ ì˜ˆì¸¡ ì‹œ í° error penalty

3. **Feature ì„ íƒ ë¬¸ì œ**:
   - ë„ˆë¬´ ë§ì€ descriptor: noise í¬í•¨
   - ë„ˆë¬´ ì ì€ descriptor: ì •ë³´ ë¶€ì¡±

4. **ì •ê·œí™” íš¨ê³¼**:
   - Ridge, Lasso ë“± ì„ í˜• ëª¨ë¸ì—ì„œ ì‹¬í•¨
   - ê³„ìˆ˜ shrinkageê°€ ì˜ˆì¸¡ ë²”ìœ„ ì¶•ì†Œ

### XGBoostê°€ ì„±ê³µí•œ ì´ìœ 

1. **Gradient Boosting ë©”ì»¤ë‹ˆì¦˜**:
   - ì”ì°¨ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ê·¹ë‹¨ê°’ í¬ì°©
   
2. **ì ì ˆí•œ ë³µì¡ë„** (30 descriptors):
   - ì¶©ë¶„í•œ ì •ë³´ + overfitting ë°©ì§€
   
3. **Tree ê¸°ë°˜ ì¥ì **:
   - ë¹„ì„ í˜• ê´€ê³„ í¬ì°©
   - Feature interaction ìë™ í•™ìŠµ

4. **Regularization íŒŒë¼ë¯¸í„°**:
   - max_depth=4: ë„ˆë¬´ ê¹Šì§€ ì•Šì•„ ì¼ë°˜í™”
   - learning_rate=0.1: ì•ˆì •ì  í•™ìŠµ

---

## ğŸ“ˆ ê°œì„  ê¶Œì¥ì‚¬í•­

### ì¦‰ì‹œ ì ìš© ê°€ëŠ¥

1. **XGBoost 30 descriptor ëª¨ë¸ ì‚¬ìš©**
   - í˜„ì¬ ìµœê³  ì„±ëŠ¥ (RÂ² = 0.78)
   
2. **Stratified Split ì ìš©**
   ```python
   from sklearn.model_selection import train_test_split
   
   # pLeachë¥¼ binningí•˜ì—¬ stratify
   y_binned = pd.qcut(y, q=5, labels=False)
   X_train, X_test, y_train, y_test = train_test_split(
       X, y, test_size=0.2, stratify=y_binned, random_state=42
   )
   ```

3. **Ensemble ì ìš©**
   - XGBoost (30D) + RandomForest (15D) + ExtraTrees (50D) ì•™ìƒë¸”
   - ê° ëª¨ë¸ì˜ ê°•ì  ê²°í•©

### ì¤‘ê¸° ê°œì„ 

4. **Quantile Regression**
   ```python
   from sklearn.ensemble import GradientBoostingRegressor
   
   # ìƒìœ„/í•˜ìœ„ quantile ë™ì‹œ í•™ìŠµ
   model_90 = GradientBoostingRegressor(loss='quantile', alpha=0.9)
   model_10 = GradientBoostingRegressor(loss='quantile', alpha=0.1)
   ```

5. **Sample Weighting**
   ```python
   # ê·¹ë‹¨ê°’ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
   sample_weights = np.ones(len(y))
   sample_weights[y < np.percentile(y, 20)] = 3.0
   sample_weights[y > np.percentile(y, 80)] = 3.0
   
   model.fit(X_train, y_train, sample_weight=sample_weights)
   ```

6. **Feature Engineering**
   - Descriptor ê°„ interaction terms
   - Polynomial features (degree=2)
   - Domain knowledge ê¸°ë°˜ composite descriptors

### ì¥ê¸° ì „ëµ

7. **ë°ì´í„° ì¦ê°•**
   - SMOTE for regression
   - Gaussian noise addition
   - ìœ ì‚¬ í™”í•©ë¬¼ ë°ì´í„°ë² ì´ìŠ¤ í†µí•©

8. **Deep Learning**
   - Neural network with extreme value loss
   - Graph Neural Network (ë¶„ì êµ¬ì¡° í™œìš©)

9. **Semi-supervised Learning**
   - 90M compoundsì˜ unlabeled data í™œìš©
   - Pseudo-labeling with high-confidence predictions

---

## ğŸ¯ ê²°ë¡ 

1. **XGBoost 30 descriptorsê°€ ìµœì **: RÂ² = 0.78 ë‹¬ì„±
2. **K-foldëŠ” ê³¼ì í•© ê²½í–¥**, Hold-outì´ ë” í˜„ì‹¤ì 
3. **ê·¹ë‹¨ê°’ ì˜ˆì¸¡ì€ ì—¬ì „íˆ ë„ì „ ê³¼ì œ** (MAE = 1.45~2.0)
4. **Descriptor ìˆ˜ì˜ ìµœì í™” ì¤‘ìš”**: 30ê°œê°€ sweet spot
5. **ë‹¤ìŒ ë‹¨ê³„**: Stratified sampling + Sample weighting + Ensemble

---

## ğŸ“ ìƒì„±ëœ íŒŒì¼

1. `model_comparison_results.csv` - ì „ì²´ ì‹¤í—˜ ê²°ê³¼
2. `detailed_results.json` - ìƒì„¸ ì •ë³´ (ì‚¬ìš©ëœ descriptors í¬í•¨)
3. `comprehensive_analysis.png` - 8ê°œ ì‹œê°í™” í¬í•¨ ì¢…í•© ë¶„ì„
4. `detailed_summary_stats.csv` - ëª¨ë¸ë³„ í†µê³„
5. `best_configurations.csv` - Top 20 ì„¤ì •

---

**ì‹¤í—˜ ì™„ë£Œ ì‹œê°„**: ì•½ 10ë¶„
**ì´ í•™ìŠµ ëª¨ë¸ ìˆ˜**: 192ê°œ (best 64ê°œ ê¸°ë¡)
**ìµœì¢… ê¶Œì¥ ëª¨ë¸**: XGBoost with 30 randomly selected cluster representatives
