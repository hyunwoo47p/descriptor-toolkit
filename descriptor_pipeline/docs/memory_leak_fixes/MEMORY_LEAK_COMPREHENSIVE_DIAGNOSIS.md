# ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì¢…í•© ì§„ë‹¨ ë° í•´ê²° ë°©ì•ˆ

## ğŸ” ë°œê²¬ëœ ë¬¸ì œë“¤

### 1. **ì¤‘ë³µ í•¨ìˆ˜ ì •ì˜ (parquet_reader_duckdb.py)**

**ìœ„ì¹˜**: `descriptor_pipeline/io/parquet_reader_duckdb.py`

**ë¬¸ì œ**:
```python
# ë¼ì¸ 26-208: ì²« ë²ˆì§¸ iter_batches_duckdb ì •ì˜
def iter_batches_duckdb(...):
    ...

# ë¼ì¸ 226-407: ë‘ ë²ˆì§¸ iter_batches_duckdb ì •ì˜ (ì™„ì „íˆ ë™ì¼)
def iter_batches_duckdb(...):
    ...
```

**ì˜í–¥**: 
- Pythonì€ ë§ˆì§€ë§‰ ì •ì˜ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ ì²« ë²ˆì§¸ ì •ì˜ëŠ” ë¬´ì‹œë¨
- ì½”ë“œ í˜¼ë€ê³¼ ìœ ì§€ë³´ìˆ˜ ë¬¸ì œ
- ì§ì ‘ì ì¸ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì›ì¸ì€ ì•„ë‹ˆì§€ë§Œ ì½”ë“œ í’ˆì§ˆ ì €í•˜

**í•´ê²°**:
```python
# ë¼ì¸ 226-407ì˜ ì¤‘ë³µ ì •ì˜ ì‚­ì œ í•„ìš”
```

---

### 2. **íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ í˜¸ì¶œ ì¸ì ë¶ˆì¼ì¹˜ (pipeline.py)**

**ìœ„ì¹˜**: `descriptor_pipeline/core/pipeline.py` ë¼ì¸ 121-125

**ë¬¸ì œ**:
```python
# pipeline.pyì—ì„œ
spearman_pass.process(
    data, columns_p1, G_spearman, self.graph_builder, self.leiden  # 5ê°œ ì¸ì
)

# advanced_filtering.pyì—ì„œ
def process(self, data: np.ndarray, columns: List[str],
           G_spearman: np.ndarray, stats: Dict):  # 4ê°œ ì¸ìë§Œ ë°›ìŒ
```

**ì˜í–¥**:
- TypeError ë°œìƒ ì˜ˆìƒ: `process() takes 5 positional arguments but 6 were given`
- íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨

**í•´ê²°**:
```python
# pipeline.py ë¼ì¸ 123-124 ìˆ˜ì •
columns_p2, spearman_info, indices_p2 = spearman_pass.process(
    data, columns_p1, G_spearman, stats_p1  # stats_p1ìœ¼ë¡œ ìˆ˜ì •
)
```

---

### 3. **DuckDB ì—°ê²° ì¬ì‚¬ìš© ëˆ„ìˆ˜**

**ìœ„ì¹˜**: `descriptor_pipeline/io/parquet_reader_duckdb.py` ë¼ì¸ 54

**ë¬¸ì œ**:
```python
def iter_batches_duckdb(...):
    global_offset = 0
    
    # ë§¤ë²ˆ ìƒˆ ì—°ê²° ìƒì„±
    conn = duckdb.connect(':memory:')
```

**ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì‹œë‚˜ë¦¬ì˜¤**:
- ì´ í•¨ìˆ˜ê°€ generatorë¡œì„œ yield ì¤‘ê°„ì— ì¤‘ë‹¨ë˜ë©´ `finally` ë¸”ë¡ì´ ì‹¤í–‰ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
- DuckDB ì—°ê²°ì´ ì œëŒ€ë¡œ ë‹«íˆì§€ ì•Šìœ¼ë©´ ë©”ëª¨ë¦¬ì— ëˆ„ì 
- ë°˜ë³µ í˜¸ì¶œ ì‹œ ì—°ê²° ê°ì²´ê°€ ìŒ“ì„

**í•´ê²°**:
```python
def iter_batches_duckdb(...):
    conn = None
    try:
        conn = duckdb.connect(':memory:')
        # ... ê¸°ì¡´ ì½”ë“œ
        
        # ê° yield í›„ ëª…ì‹œì  ìºì‹œ ì •ë¦¬
        for ...:
            yield X, global_offset
            
            # ì¦‰ì‹œ ì •ë¦¬
            del X
            gc.collect()
            
    finally:
        if conn is not None:
            try:
                conn.close()
            except:
                pass
        gc.collect()
```

---

### 4. **DataFrame ë³€í™˜ ì‹œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜**

**ìœ„ì¹˜**: `parquet_reader_duckdb.py` ì—¬ëŸ¬ ìœ„ì¹˜

**ë¬¸ì œ**:
```python
df_batch = conn.execute(batch_query).fetch_df()  # pandas DataFrame ìƒì„±

# NumPyë¡œ ë³€í™˜
X = df_batch[columns].values.astype(np.float64)
del df_batch  # ì‚­ì œí•˜ì§€ë§Œ pandasì˜ ë‚´ë¶€ ìºì‹œëŠ” ë‚¨ì„ ìˆ˜ ìˆìŒ
```

**ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì›ì¸**:
- pandas DataFrameì€ ë‚´ë¶€ì ìœ¼ë¡œ BlockManagerë¥¼ ì‚¬ìš©
- `.values`ëŠ” viewë¥¼ ë°˜í™˜í•  ìˆ˜ ìˆì–´ ì›ë³¸ ì°¸ì¡° ìœ ì§€
- `del df_batch`ë¡œ ì‚­ì œí•´ë„ NumPy arrayê°€ pandas ë©”ëª¨ë¦¬ë¥¼ ì°¸ì¡°í•  ìˆ˜ ìˆìŒ

**í•´ê²°**:
```python
df_batch = conn.execute(batch_query).fetch_df()

# ëª…ì‹œì  ë³µì‚¬ë¡œ ë…ë¦½ì„± ë³´ì¥
X = df_batch[columns].values.copy().astype(np.float64)

# pandas ê°ì²´ ì™„ì „ ì‚­ì œ
del df_batch
gc.collect()
```

---

### 5. **GPU í…ì„œ ëˆ„ìˆ˜**

**ìœ„ì¹˜**: `descriptor_pipeline/core/similarity_gpu.py`

**ë¬¸ì œ**:
```python
@torch.no_grad()
def _compute_correlation_matrix_gpu(self, X: np.ndarray):
    X_gpu = torch.from_numpy(X).to(self.device, dtype=torch.float32)
    
    # ... ê³„ì‚°
    
    G_cpu = G.cpu().numpy()
    del X_gpu, G
    torch.cuda.empty_cache()
```

**ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì›ì¸**:
- `torch.no_grad()` ì»¨í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œëŠ” ë¶ˆì¶©ë¶„
- ì¤‘ê°„ í…ì„œë“¤ì´ ê³„ì‚° ê·¸ë˜í”„ì— ë‚¨ì„ ìˆ˜ ìˆìŒ
- `.cpu().numpy()` ë³€í™˜ ì‹œ GPU ë©”ëª¨ë¦¬ ì°¸ì¡° ìœ ì§€ ê°€ëŠ¥

**í•´ê²°**:
```python
@torch.no_grad()
def _compute_correlation_matrix_gpu(self, X: np.ndarray):
    try:
        X_gpu = torch.from_numpy(X).to(self.device, dtype=torch.float32)
        
        # ê³„ì‚° (ì¤‘ê°„ ë³€ìˆ˜ ì¦‰ì‹œ ì‚­ì œ)
        # ...
        
        # GPU -> CPU ë³€í™˜ (ëª…ì‹œì  ë³µì‚¬)
        G_cpu = G.detach().cpu().numpy().copy()
        
    finally:
        # ëª¨ë“  GPU í…ì„œ ì‚­ì œ
        if 'X_gpu' in locals():
            del X_gpu
        if 'G' in locals():
            del G
        
        # GPU ë©”ëª¨ë¦¬ ê°•ì œ í•´ì œ
        torch.cuda.empty_cache()
        if hasattr(torch.cuda, 'reset_peak_memory_stats'):
            torch.cuda.reset_peak_memory_stats()
```

---

### 6. **NumPy ë°°ì—´ view ì°¸ì¡° ëˆ„ìˆ˜**

**ìœ„ì¹˜**: ì—¬ëŸ¬ íŒŒì¼ì—ì„œ ë°°ì—´ ìŠ¬ë¼ì´ì‹±

**ë¬¸ì œ**:
```python
# pipeline.py
data_p2 = data[:, indices_p2]  # view ìƒì„±, ì›ë³¸ data ì°¸ì¡° ìœ ì§€
data_p3 = data[:, indices_p3]  # ë˜ ë‹¤ë¥¸ view

# ì›ë³¸ dataê°€ í•´ì œë˜ì§€ ì•ŠìŒ
```

**ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì›ì¸**:
- NumPy slicingì€ ê¸°ë³¸ì ìœ¼ë¡œ view ë°˜í™˜
- ViewëŠ” ì›ë³¸ ë°°ì—´ ì „ì²´ë¥¼ ë©”ëª¨ë¦¬ì— ìœ ì§€
- Passë§ˆë‹¤ ìƒˆë¡œìš´ view ìƒì„± â†’ ì›ë³¸ ë©”ëª¨ë¦¬ ê³„ì† ì ìœ 

**í•´ê²°**:
```python
# ëª…ì‹œì  ë³µì‚¬ë¡œ ë…ë¦½ì„± í™•ë³´
data_p2 = data[:, indices_p2].copy()

# ì›ë³¸ ëª…ì‹œì  ì‚­ì œ
del data
gc.collect()

# ë‹¤ìŒ pass
data_p3 = data_p2[:, indices_p3_sub].copy()
del data_p2
gc.collect()
```

---

### 7. **í†µê³„ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ëˆ„ì **

**ìœ„ì¹˜**: `pass1_statistics.py` ë° pipelineì—ì„œ stats ì „ë‹¬

**ë¬¸ì œ**:
```python
# statsì— ëŒ€ëŸ‰ì˜ ë°ì´í„° ì €ì¥
stats = {
    'means': np.array(...),      # (p,)
    'stds': np.array(...),        # (p,)
    'cdf_lookups': [...],         # pê°œì˜ ë¦¬ìŠ¤íŠ¸
    'missing_rates': ...,
    # ... ê³„ì† ì¶”ê°€
}

# ë§¤ passë§ˆë‹¤ ë³µì‚¬/ì „ë‹¬ë˜ì§€ë§Œ ì´ì „ statsëŠ” ì‚­ì œ ì•ˆë¨
```

**ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì›ì¸**:
- CDF lookup í…Œì´ë¸”ì´ ê° descriptorë§ˆë‹¤ ì €ì¥ (ë©”ëª¨ë¦¬ ì§‘ì•½ì )
- Passë§ˆë‹¤ statsë¥¼ í•„í„°ë§í•˜ì§€ë§Œ ì´ì „ ë²„ì „ì´ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆìŒ
- ë”•ì…”ë„ˆë¦¬ ë‚´ë¶€ì˜ NumPy ë°°ì—´ë“¤ì´ ì°¸ì¡° ìœ ì§€

**í•´ê²°**:
```python
def _filter_stats_by_indices(self, stats: Dict, indices: np.ndarray) -> Dict:
    """Filter statistics by indices with explicit memory management"""
    stats_filtered = {}
    
    for key, value in stats.items():
        if isinstance(value, np.ndarray) and value.ndim >= 1:
            try:
                # ëª…ì‹œì  ë³µì‚¬
                stats_filtered[key] = value[indices].copy()
            except:
                stats_filtered[key] = value
        elif isinstance(value, list):
            # ë¦¬ìŠ¤íŠ¸ë„ í•„í„°ë§
            try:
                stats_filtered[key] = [value[i] for i in indices]
            except:
                stats_filtered[key] = value
        else:
            stats_filtered[key] = value
    
    # ì´ì „ stats ëª…ì‹œì  ì‚­ì œ
    return stats_filtered
```

---

### 8. **iter_batches ìˆœí™˜ ì°¸ì¡°**

**ìœ„ì¹˜**: `descriptor_pipeline/io/__init__.py` ë° ì‚¬ìš©ì²˜

**ë¬¸ì œ**:
```python
# iter_batchesê°€ generatorë¡œ êµ¬í˜„ë˜ì–´ ìˆìœ¼ë©´
for batch_data, offset in iter_batches(...):
    batches.append(batch_data)
    # batch_dataê°€ ë¦¬ìŠ¤íŠ¸ì— ëˆ„ì ë˜ë©´ì„œ
    # generator ë‚´ë¶€ ìƒíƒœë„ ìœ ì§€ë¨
```

**ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì›ì¸**:
- Generatorê°€ ì™„ì „íˆ ì†Œì§„ë˜ì§€ ì•Šìœ¼ë©´ ë‚´ë¶€ ìƒíƒœ ìœ ì§€
- Generator ë‚´ë¶€ì˜ DuckDB ì—°ê²°ì´ë‚˜ ì¤‘ê°„ ë³€ìˆ˜ë“¤ì´ ë©”ëª¨ë¦¬ì— ë‚¨ìŒ
- Exception ë°œìƒ ì‹œ generator cleanup ì•ˆë¨

**í•´ê²°**:
```python
# iter_batches ì‚¬ìš© ì‹œ ëª…ì‹œì  cleanup
def _load_data(self, parquet_paths: List[str], columns: List[str]) -> np.ndarray:
    """Load data into memory with explicit cleanup"""
    batches = []
    batch_generator = None
    
    try:
        batch_generator = iter_batches(parquet_paths, columns, self.config.batch_rows)
        
        for batch_data, offset in batch_generator:
            # ëª…ì‹œì  ë³µì‚¬
            batches.append(batch_data.copy())
            del batch_data
            
    finally:
        # Generator ëª…ì‹œì  ì¢…ë£Œ
        if batch_generator is not None:
            try:
                batch_generator.close()
            except:
                pass
        
        gc.collect()
    
    result = np.vstack(batches)
    
    # ì¤‘ê°„ ë¦¬ìŠ¤íŠ¸ ì‚­ì œ
    del batches
    gc.collect()
    
    return result
```

---

## ğŸ”§ ì¶”ê°€ ë©”ëª¨ë¦¬ ê´€ë¦¬ ê¶Œì¥ì‚¬í•­

### 9. **Pass ê°„ ëª…ì‹œì  ë©”ëª¨ë¦¬ ì •ë¦¬**

**ìœ„ì¹˜**: `pipeline.py`ì˜ ê° pass ì‚¬ì´

**ì¶”ê°€ í•„ìš”**:
```python
# Pass 1 ì™„ë£Œ í›„
columns_p1, stats_p1, indices_p1 = self.pass1.compute(...)

# ëª…ì‹œì  ì •ë¦¬
import gc
gc.collect()

if self.using_gpu:
    torch.cuda.empty_cache()
    torch.cuda.synchronize()

# Pass 2 ì‹œì‘
```

---

### 10. **Context Manager ì‚¬ìš©**

**ìœ„ì¹˜**: GPU ì—°ì‚°ì´ ìˆëŠ” ëª¨ë“  í•¨ìˆ˜

**ì¶”ê°€ í•„ìš”**:
```python
class GPUContext:
    """GPU ë©”ëª¨ë¦¬ ì•ˆì „ ì»¨í…ìŠ¤íŠ¸"""
    def __init__(self, device):
        self.device = device
    
    def __enter__(self):
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()

# ì‚¬ìš©
with GPUContext(self.device):
    G_spearman = spearman_gpu.compute(...)
```

---

## ğŸ“‹ Import ë° ë³€ìˆ˜ í˜¸ì¶œ ë¬¸ì œ ì²´í¬ë¦¬ìŠ¤íŠ¸

### âœ… í™•ì¸ëœ Import ë¬¸ì œ:

1. **pipeline.py ë¼ì¸ 22**: 
```python
from descriptor_pipeline.io import iter_batches
```
â†’ `parquet_reader_duckdb.py`ì˜ `iter_batches_duckdb` ì‚¬ìš©í•´ì•¼ í•¨
â†’ ë˜ëŠ” `__init__.py`ì—ì„œ ì œëŒ€ë¡œ export ë˜ëŠ”ì§€ í™•ì¸ í•„ìš”

2. **advanced_filtering.py**:
```python
# í˜„ì¬ëŠ” ë¬¸ì œ ì—†ìŒ - ëª¨ë“  import ì •ìƒ
```

3. **similarity_gpu.py**:
```python
# ì²´í¬ í•„ìš”: memory_leak_patchê°€ ì‹¤ì œë¡œ importë˜ëŠ”ì§€
# pipeline.pyë‚˜ __main__.pyì—ì„œ ëª…ì‹œì  import í•„ìš”
```

---

## ğŸš€ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ìˆ˜ì •ì‚¬í•­

### ìš°ì„ ìˆœìœ„ 1 (Critical - ì¦‰ì‹œ ìˆ˜ì •):
1. `pipeline.py` ë¼ì¸ 123-124: í•¨ìˆ˜ í˜¸ì¶œ ì¸ì ìˆ˜ì •
2. `parquet_reader_duckdb.py`: ì¤‘ë³µ í•¨ìˆ˜ ì •ì˜ ì œê±° (ë¼ì¸ 226-407 ì‚­ì œ)
3. `parquet_reader_duckdb.py`: `.copy()` ì¶”ê°€í•˜ì—¬ view ì°¸ì¡° ë°©ì§€

### ìš°ì„ ìˆœìœ„ 2 (Important - ë¹ ë¥¸ ì‹œì¼ ë‚´ ìˆ˜ì •):
4. `pipeline.py`: NumPy view â†’ copyë¡œ ë³€ê²½
5. `similarity_gpu.py`: `.detach().cpu().numpy().copy()` íŒ¨í„´ ì ìš©
6. Pass ê°„ ëª…ì‹œì  `gc.collect()` ì¶”ê°€

### ìš°ì„ ìˆœìœ„ 3 (Recommended - ì ì§„ì  ê°œì„ ):
7. Context Manager ì¶”ê°€
8. Generator cleanup ê°œì„ 
9. í†µê³„ ì •ë³´ í•„í„°ë§ ê°œì„ 

---

## ğŸ§ª ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê²€ì¦ ë°©ë²•

```python
import tracemalloc
import gc

# í…ŒìŠ¤íŠ¸ ì‹œì‘
tracemalloc.start()
gc.collect()

snapshot1 = tracemalloc.take_snapshot()

# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
pipeline.run()

gc.collect()
snapshot2 = tracemalloc.take_snapshot()

# ë©”ëª¨ë¦¬ ì¦ê°€ ë¶„ì„
top_stats = snapshot2.compare_to(snapshot1, 'lineno')

print("\n[ Top 10 Memory Increases ]")
for stat in top_stats[:10]:
    print(stat)
```

---

## ê²°ë¡ 

**ì£¼ìš” ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì›ì¸ ìš”ì•½**:
1. NumPy array view ì°¸ì¡° (ê°€ì¥ í° ì›ì¸)
2. DuckDB DataFrame â†’ NumPy ë³€í™˜ ì‹œ ì°¸ì¡° ìœ ì§€
3. GPU í…ì„œ ë¶ˆì™„ì „í•œ í•´ì œ
4. Generator ë‚´ë¶€ ìƒíƒœ ìœ ì§€
5. í†µê³„ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ëˆ„ì 

**í•´ê²° í•µì‹¬**:
- ëª¨ë“  ë°°ì—´ ë³€í™˜ì— `.copy()` ì¶”ê°€
- ê° pass í›„ ëª…ì‹œì  `del` + `gc.collect()`
- GPU ì‚¬ìš© í›„ `.detach()` ì¶”ê°€
- Context Managerë¡œ ìë™ ì •ë¦¬
