# ì‹¤ì „ ì ìš© ê°€ì´ë“œ: ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ìˆ˜ì •

## ğŸ¯ ì¦‰ì‹œ ì ìš©í•  ìˆ˜ì •ì‚¬í•­ (ìš°ì„ ìˆœìœ„ë³„)

### Priority 1: Critical Fixes (ì¦‰ì‹œ ìˆ˜ì • í•„ìˆ˜)

#### 1. `descriptor_pipeline/io/parquet_reader_duckdb.py`

**ë¬¸ì œ 1: ì¤‘ë³µ í•¨ìˆ˜ ì •ì˜**
```python
# ë¼ì¸ 226-407 ì‚­ì œ
# iter_batches_duckdb í•¨ìˆ˜ê°€ ë‘ ë²ˆ ì •ì˜ë˜ì–´ ìˆìŒ
```

**ë¬¸ì œ 2: DataFrame view ì°¸ì¡°**
```python
# ê¸°ì¡´ (ë¼ì¸ 103, 156, 193, 356, 393)
X = df_batch[columns].values.astype(np.float64)

# ìˆ˜ì • í›„
X = df_batch[columns].values.copy().astype(np.float64)
```

**ì „ì²´ ìˆ˜ì • ìœ„ì¹˜**:
- ë¼ì¸ 103: ì²« ë²ˆì§¸ ì‚¬ìš©
- ë¼ì¸ 156: ìƒ˜í”Œë§ ëª¨ë“œ - ì „ì²´ ì‚¬ìš©
- ë¼ì¸ 193: ìƒ˜í”Œë§ ëª¨ë“œ - ìƒ˜í”Œë§
- ë¼ì¸ 226-407: ì „ì²´ ì‚­ì œ (ì¤‘ë³µ ì •ì˜)

---

#### 2. `descriptor_pipeline/core/pipeline.py`

**ë¬¸ì œ 1: í•¨ìˆ˜ í˜¸ì¶œ ì¸ì ë¶ˆì¼ì¹˜ (ë¼ì¸ 123-125)**
```python
# ê¸°ì¡´ (ì˜¤ë¥˜ ë°œìƒ)
columns_p2, spearman_info, indices_p2 = spearman_pass.process(
    data, columns_p1, G_spearman, self.graph_builder, self.leiden
)

# ìˆ˜ì • í›„
columns_p2, spearman_info, indices_p2 = spearman_pass.process(
    data, columns_p1, G_spearman, stats_p1
)
```

**ë¬¸ì œ 2: NumPy view ì°¸ì¡° (ë¼ì¸ 142-143)**
```python
# ê¸°ì¡´
data_p2 = data[:, indices_p2]
G_spearman_p2 = G_spearman[indices_p2][:, indices_p2]

# ìˆ˜ì • í›„
data_p2 = data[:, indices_p2].copy()
G_spearman_p2 = G_spearman[indices_p2][:, indices_p2].copy()

# ì›ë³¸ ì‚­ì œ ì¶”ê°€
del data, G_spearman
gc.collect()
```

**ë¬¸ì œ 3: Pass 3 ë°ì´í„° ì¤€ë¹„ (ë¼ì¸ 169)**
```python
# ê¸°ì¡´
data_p3 = data[:, indices_p3]

# ìˆ˜ì • í›„
data_p3 = data_p2[:, indices_p3_sub].copy()

# ì´ì „ ë°ì´í„° ì‚­ì œ
del data_p2
gc.collect()
```

**ì „ì²´ ìˆ˜ì • ìœ„ì¹˜**:
- ë¼ì¸ 22: `import gc` ì¶”ê°€
- ë¼ì¸ 123-125: í•¨ìˆ˜ í˜¸ì¶œ ìˆ˜ì •
- ë¼ì¸ 142-143: .copy() ì¶”ê°€ + ì›ë³¸ ì‚­ì œ
- ë¼ì¸ 169: .copy() ì¶”ê°€ + ì›ë³¸ ì‚­ì œ
- ë¼ì¸ 96, 108, 134, 161, 204 ë“±: `gc.collect()` ì¶”ê°€

---

#### 3. `descriptor_pipeline/core/similarity_gpu.py`

**ë¬¸ì œ: GPU í…ì„œ ë³€í™˜ ì‹œ ì°¸ì¡° ìœ ì§€**

**3ê°œ ìœ„ì¹˜ ìˆ˜ì • í•„ìš”**:

**ìœ„ì¹˜ 1: ë¼ì¸ 158 (Spearman)**
```python
# ê¸°ì¡´
G_cpu = G.cpu().numpy()

# ìˆ˜ì • í›„
G_cpu = G.detach().cpu().numpy().copy()
```

**ìœ„ì¹˜ 2: ë¼ì¸ 390 (HSIC)**
```python
# ê¸°ì¡´
H_cpu = H.cpu().numpy()

# ìˆ˜ì • í›„
H_cpu = H.detach().cpu().numpy().copy()
```

**ìœ„ì¹˜ 3: ë¼ì¸ 634 (RDC)**
```python
# ê¸°ì¡´
R_cpu = R.cpu().numpy()

# ìˆ˜ì • í›„
R_cpu = R.detach().cpu().numpy().copy()
```

---

### Priority 2: Important Memory Management

#### 4. `pipeline.py` - ëª…ì‹œì  ë©”ëª¨ë¦¬ ì •ë¦¬ ë©”ì„œë“œ ì¶”ê°€

**ì¶”ê°€ ìœ„ì¹˜: í´ë˜ìŠ¤ ë‚´ë¶€ (ë¼ì¸ 60 ì´í›„)**
```python
def _cleanup_memory(self):
    """ëª…ì‹œì  ë©”ëª¨ë¦¬ ì •ë¦¬"""
    import gc
    gc.collect()
    if self.using_gpu:
        import torch
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
```

**ì‚¬ìš© ìœ„ì¹˜ (ê° Pass í›„)**:
```python
# Pass 0 í›„
self._cleanup_memory()

# Pass 1 í›„  
self._cleanup_memory()

# Pass 2 í›„
self._cleanup_memory()

# Pass 3 í›„
self._cleanup_memory()

# Pass 4 í›„
self._cleanup_memory()
```

---

#### 5. `pipeline.py` - _filter_stats_by_indices ìˆ˜ì • (ë¼ì¸ 253-264)

```python
def _filter_stats_by_indices(self, stats: Dict, indices: np.ndarray) -> Dict:
    """Filter statistics by indices with explicit copy"""
    stats_filtered = {}
    
    for key, value in stats.items():
        if isinstance(value, np.ndarray) and value.ndim >= 1:
            try:
                # FIXED: .copy() ì¶”ê°€
                stats_filtered[key] = value[indices].copy()
            except:
                stats_filtered[key] = value
        elif isinstance(value, list):
            # ë¦¬ìŠ¤íŠ¸ë„ í•„í„°ë§
            try:
                stats_filtered[key] = [value[i] for i in indices]
            except:
                stats_filtered[key] = value
        else:
            stats_filtered[key] = value
    
    return stats_filtered
```

---

#### 6. `pipeline.py` - _load_data ê°œì„  (ë¼ì¸ 246-251)

```python
def _load_data(self, parquet_paths: List[str], columns: List[str]) -> np.ndarray:
    """Load data into memory with explicit cleanup"""
    batches = []
    batch_generator = None
    
    try:
        batch_generator = iter_batches(parquet_paths, columns, self.config.batch_rows)
        
        for batch_data, offset in batch_generator:
            # ëª…ì‹œì  ë³µì‚¬ë¡œ generator ì°¸ì¡° ì œê±°
            batches.append(batch_data.copy())
            del batch_data
    
    finally:
        # Generator cleanup
        if batch_generator is not None:
            try:
                batch_generator.close()
            except:
                pass
        
        gc.collect()
    
    result = np.vstack(batches)
    
    # ì¤‘ê°„ ë¦¬ìŠ¤íŠ¸ ì‚­ì œ
    del batches
    gc.collect()
    
    return result
```

---

### Priority 3: Optional Enhancements

#### 7. Context Manager for GPU Operations

**ìƒˆ íŒŒì¼ ìƒì„±: `descriptor_pipeline/utils/memory.py`**
```python
"""
Memory management utilities
"""

import gc
import torch
from typing import Optional


class GPUMemoryContext:
    """GPU ë©”ëª¨ë¦¬ ì•ˆì „ ì»¨í…ìŠ¤íŠ¸"""
    
    def __init__(self, device: Optional[torch.device] = None):
        self.device = device
    
    def __enter__(self):
        if self.device is not None and self.device.type == 'cuda':
            torch.cuda.empty_cache()
        gc.collect()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.device is not None and self.device.type == 'cuda':
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
        return False


def cleanup_memory(device: Optional[torch.device] = None):
    """ëª…ì‹œì  ë©”ëª¨ë¦¬ ì •ë¦¬"""
    gc.collect()
    if device is not None and device.type == 'cuda':
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
```

**ì‚¬ìš© ì˜ˆì‹œ (pipeline.py)**:
```python
from descriptor_pipeline.utils.memory import GPUMemoryContext

# GPU ì—°ì‚° ì „í›„ ìë™ ì •ë¦¬
with GPUMemoryContext(self.device):
    G_spearman = spearman_gpu.compute(parquet_paths, columns_p1, stats_p1)
```

---

## ğŸ”§ ìˆ˜ì • ìˆœì„œ (ì‹¤ì „ ì ìš© ì‹œ)

### Step 1: ë°±ì—…
```bash
# í˜„ì¬ ì½”ë“œ ë°±ì—…
cp -r descriptor_pipeline descriptor_pipeline_backup_$(date +%Y%m%d_%H%M%S)
```

### Step 2: Critical Fixes (ìˆœì„œëŒ€ë¡œ)
1. `parquet_reader_duckdb.py` ìˆ˜ì •
2. `pipeline.py` í•¨ìˆ˜ í˜¸ì¶œ ì¸ì ìˆ˜ì •
3. `pipeline.py` NumPy view ìˆ˜ì •
4. `similarity_gpu.py` GPU í…ì„œ ë³€í™˜ ìˆ˜ì •

### Step 3: í…ŒìŠ¤íŠ¸
```bash
# ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
python -m descriptor_pipeline.cli.run_pipeline \
    --parquet-glob "data/test_*.parquet" \
    --output-dir "output/test" \
    --checkpoint
```

### Step 4: Memory Profiling
```python
import tracemalloc
tracemalloc.start()

# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
pipeline.run()

# ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·
snapshot = tracemalloc.take_snapshot()
top_stats = snapshot.statistics('lineno')

for stat in top_stats[:10]:
    print(stat)
```

### Step 5: Important Memory Management ì ìš©
- `_cleanup_memory()` ë©”ì„œë“œ ì¶”ê°€
- `_filter_stats_by_indices()` ìˆ˜ì •
- `_load_data()` ê°œì„ 

### Step 6: ì „ì²´ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸
```bash
# ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§í•˜ë©´ì„œ ì‹¤í–‰
watch -n 1 'nvidia-smi && free -h'

# ì‹¤ì œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
python -m descriptor_pipeline.cli.run_pipeline \
    --parquet-glob "data/full_*.parquet" \
    --output-dir "output/full" \
    --prefer-gpu \
    --checkpoint \
    --verbose
```

---

## ğŸ“Š ìˆ˜ì • ì „í›„ ì˜ˆìƒ íš¨ê³¼

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°œì„  ì˜ˆìƒ
- **Before**: iterationë§ˆë‹¤ 1-2GB ì¦ê°€
- **After**: iterationë§ˆë‹¤ ìµœëŒ€ 100-200MB ì¦ê°€ (ì •ìƒ ë²”ìœ„)

### ì£¼ìš” ê°œì„ ì‚¬í•­
1. **NumPy view ì œê±°**: ê°€ì¥ í° ëˆ„ìˆ˜ ì›ì¸ í•´ê²°
2. **GPU í…ì„œ ì°¸ì¡° ì œê±°**: GPU ë©”ëª¨ë¦¬ ì•ˆì •í™”
3. **DataFrame ì°¸ì¡° ì œê±°**: pandas ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€
4. **ëª…ì‹œì  cleanup**: ê° pass í›„ ì¦‰ì‹œ ë©”ëª¨ë¦¬ í•´ì œ

---

## ğŸ› ë””ë²„ê¹… Tips

### ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í™•ì¸
```python
import psutil
import os

process = psutil.Process(os.getpid())

# iteration ì „
mem_before = process.memory_info().rss / 1024 / 1024  # MB
print(f"Memory before: {mem_before:.1f} MB")

# iteration ì‹¤í–‰
# ...

# iteration í›„
mem_after = process.memory_info().rss / 1024 / 1024  # MB
print(f"Memory after: {mem_after:.1f} MB")
print(f"Leaked: {mem_after - mem_before:.1f} MB")
```

### GPU ë©”ëª¨ë¦¬ í™•ì¸
```python
if torch.cuda.is_available():
    allocated = torch.cuda.memory_allocated() / 1024 / 1024  # MB
    reserved = torch.cuda.memory_reserved() / 1024 / 1024  # MB
    print(f"GPU allocated: {allocated:.1f} MB")
    print(f"GPU reserved: {reserved:.1f} MB")
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

ì ìš© ì „ í™•ì¸:
- [ ] ì½”ë“œ ë°±ì—… ì™„ë£Œ
- [ ] ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸
- [ ] í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„

Priority 1 (Critical):
- [ ] parquet_reader_duckdb.py - ì¤‘ë³µ í•¨ìˆ˜ ì‚­ì œ
- [ ] parquet_reader_duckdb.py - .copy() ì¶”ê°€ (4ê³³)
- [ ] pipeline.py - í•¨ìˆ˜ í˜¸ì¶œ ì¸ì ìˆ˜ì •
- [ ] pipeline.py - NumPy view â†’ copy (2ê³³)
- [ ] similarity_gpu.py - .detach().cpu().numpy().copy() (3ê³³)

Priority 2 (Important):
- [ ] pipeline.py - _cleanup_memory() ì¶”ê°€
- [ ] pipeline.py - Pass í›„ cleanup í˜¸ì¶œ (5ê³³)
- [ ] pipeline.py - _filter_stats_by_indices ìˆ˜ì •
- [ ] pipeline.py - _load_data ê°œì„ 

Priority 3 (Optional):
- [ ] utils/memory.py ìƒì„±
- [ ] GPUMemoryContext ì ìš©

í…ŒìŠ¤íŠ¸:
- [ ] ì‘ì€ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ í™•ì¸
- [ ] ì „ì²´ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í•´ê²° í™•ì¸

---

## ğŸš¨ ì£¼ì˜ì‚¬í•­

1. **.copy() ë‚¨ìš© ì£¼ì˜**: 
   - ë„ˆë¬´ ìì£¼ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ ì €í•˜
   - ê¼­ í•„ìš”í•œ ê³³(view ì°¸ì¡° ì œê±°)ì—ë§Œ ì‚¬ìš©

2. **gc.collect() ë‚¨ìš© ì£¼ì˜**:
   - Pass ë‹¨ìœ„ë¡œë§Œ í˜¸ì¶œ (ë°°ì¹˜ë§ˆë‹¤ëŠ” ë¹„íš¨ìœ¨)
   - GPU ì‘ì—… í›„ì—ëŠ” í•„ìˆ˜

3. **del ë¬¸ ìˆœì„œ**:
   - ì°¸ì¡° ê´€ê³„ ê³ ë ¤í•˜ì—¬ ì—­ìˆœìœ¼ë¡œ ì‚­ì œ
   - del í›„ ì¦‰ì‹œ gc.collect() í˜¸ì¶œ

4. **í…ŒìŠ¤íŠ¸ í•„ìˆ˜**:
   - ìˆ˜ì • í›„ ë°˜ë“œì‹œ ì‘ì€ ë°ì´í„°ë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸
   - ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë„êµ¬ í™œìš©

---

ì´ ê°€ì´ë“œë¥¼ ë”°ë¼ ë‹¨ê³„ì ìœ¼ë¡œ ìˆ˜ì •í•˜ë©´ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë¬¸ì œê°€ í•´ê²°ë  ê²ƒì…ë‹ˆë‹¤!
